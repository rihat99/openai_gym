{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_SAC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtCBxBbtdwtALfdKU2HVjG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rihat99/openai_gym/blob/main/SAC_half_cheetah_slow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIKnvbZ2w6l"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# !pip install box2d-py\n",
        "!pip install pybullet\n",
        "import pybullet\n",
        "import pybullet_envs\n",
        "import gym\n",
        "print(pybullet_envs.getList())\n",
        "\n",
        "env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
        "max_steps = env._max_episode_steps\n",
        "# max_steps = 1600\n",
        "# env._max_episode_steps = max_steps\n",
        "\n",
        "print(tf.__version__)\n",
        "print(gym.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw2Okwll24fS"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G0WDNN18xhq",
        "outputId": "a9f1feb1-9d34-4486-9850-c9aeb73d47e5"
      },
      "source": [
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "#num_actions = 3\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of State Space ->  26\n",
            "Size of Action Space ->  6\n",
            "Max Value of Action ->  1.0\n",
            "Min Value of Action ->  -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60HSfjf829Jt"
      },
      "source": [
        "layer_size = 256\n",
        "activation = \"elu\"\n",
        "w_bound = 3e-3\n",
        "\n",
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    #last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states))\n",
        "    out = layers.Dense(layer_size, activation=activation)(inputs)\n",
        "    out = layers.Dense(layer_size, activation=activation)(out)\n",
        "    # out = layers.Dense(layer_size//4, activation=activation)(out)\n",
        "    mean = layers.Dense(num_actions, \n",
        "                        kernel_initializer=tf.random_uniform_initializer(-w_bound, w_bound),\n",
        "                        bias_initializer=tf.random_uniform_initializer(-w_bound, w_bound)\n",
        "    )(out)\n",
        "    log_std = layers.Dense(num_actions,\n",
        "                        kernel_initializer=tf.random_uniform_initializer(-w_bound, w_bound),\n",
        "                        bias_initializer=tf.random_uniform_initializer(-w_bound, w_bound)\n",
        "    )(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    # outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, [mean, log_std])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(layer_size//2, activation=activation)(state_input)\n",
        "    state_out = layers.Dense(layer_size, activation=activation)(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(layer_size//4, activation=activation)(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(layer_size, activation=activation)(concat)\n",
        "    out = layers.Dense(layer_size, activation=activation)(out)\n",
        "    # out = layers.Dense(layer_size//4, activation=activation)(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfYdLatJyEc"
      },
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal()\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYJ0PuP83TYL"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64, alpha = 0.2):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = tf.Variable(alpha, dtype=tf.float32)\n",
        "        self.target_entropy = tf.constant(-num_actions, dtype=tf.float32)\n",
        "        self.sigma_noise = 1e-6\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "        self.done_buffer[index] = obs_tuple[4]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "\n",
        "    def policy(self, state, noise=0):\n",
        "        mu, log_sigma = actor_model(state)\n",
        "        log_sigma = tf.clip_by_value(log_sigma, -20, 2);\n",
        "\n",
        "        sigma = tf.exp(log_sigma)\n",
        "        dist = tfp.distributions.Normal(mu, sigma)\n",
        "        raw_actions = dist.sample()\n",
        "        log_action = dist.log_prob(raw_actions)\n",
        "\n",
        "        mu = tf.tanh(mu)\n",
        "        actions = tf.tanh(raw_actions)\n",
        "\n",
        "        log_action -= tf.math.log(1.0 - tf.math.pow(actions, 2) + self.sigma_noise)\n",
        "        log_action = tf.reduce_sum(log_action, axis=1, keepdims=True)\n",
        "\n",
        "        actions = (actions + noise) * upper_bound\n",
        "        mu = mu * upper_bound\n",
        "        actions = tf.clip_by_value(actions, lower_bound, upper_bound)\n",
        "        return actions, log_action, mu\n",
        "\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch,\n",
        "    ):\n",
        "        next_actions, next_log_action, _ = self.policy(next_state_batch)\n",
        "\n",
        "        target_Q_values_a = target_critic_a([next_state_batch, next_actions], training=True)\n",
        "        target_Q_values_b = target_critic_b([next_state_batch, next_actions], training=True)\n",
        "        min_target_Q_values = tf.math.minimum(target_Q_values_a, target_Q_values_b)\n",
        "        soft_Q_target = min_target_Q_values - self.alpha * next_log_action\n",
        "\n",
        "        y = reward_batch + gamma * soft_Q_target\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_value_a = critic_model_a([state_batch, action_batch], training=True)\n",
        "            critic_loss_a = tf.math.reduce_mean(tf.math.square(y - critic_value_a))\n",
        "\n",
        "        critic_grad_a = tape.gradient(critic_loss_a, critic_model_a.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad_a, critic_model_a.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_value_b = critic_model_b([state_batch, action_batch], training=True)\n",
        "            critic_loss_b = tf.math.reduce_mean(tf.math.square(y - critic_value_b))\n",
        "\n",
        "        critic_grad_b = tape.gradient(critic_loss_b, critic_model_b.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad_b, critic_model_b.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions, log_action, _ = self.policy(state_batch)\n",
        "\n",
        "            critic_value_a = critic_model_a([state_batch, actions], training=True)\n",
        "            critic_value_b = critic_model_b([state_batch, actions], training=True)\n",
        "            min_Q = tf.math.minimum(critic_value_a, critic_value_b)\n",
        "            soft_Q = self.alpha * log_action - min_Q\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = tf.reduce_mean(soft_Q)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables))\n",
        "        \n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions, log_action, _ = self.policy(state_batch)\n",
        "            # log_alpha = tf.math.log(self.alpha)\n",
        "            alpha_loss = -tf.reduce_mean(self.alpha * (log_action + self.target_entropy))\n",
        "        \n",
        "        alpha_grads = tape.gradient(alpha_loss, [self.alpha])\n",
        "        alpha_optimizer.apply_gradients(zip(alpha_grads, [self.alpha]))\n",
        "\n",
        "        return critic_loss_a, critic_loss_b, actor_loss\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
        "        done_batch = tf.cast(done_batch, dtype=tf.float32)\n",
        "\n",
        "        critic_loss_a, critic_loss_b, actor_loss = self.update(\n",
        "            state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "        )\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('train/critic_loss_a', critic_loss_a, step=total_step)\n",
        "            tf.summary.scalar('train/critic_loss_b', critic_loss_b, step=total_step)\n",
        "            tf.summary.scalar('train/actor_loss', actor_loss, step=total_step)\n",
        "            tf.summary.scalar('train/alpha', self.alpha, step=total_step)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2b6tlHe8TO6"
      },
      "source": [
        "actor_model = get_actor()\n",
        "\n",
        "critic_model_a = get_critic()\n",
        "critic_model_b = get_critic()\n",
        "\n",
        "target_critic_a = get_critic()\n",
        "target_critic_b = get_critic()\n",
        "# Making the weights equal initially\n",
        "target_critic_a.set_weights(critic_model_a.get_weights())\n",
        "target_critic_b.set_weights(critic_model_b.get_weights())\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.001\n",
        "actor_lr = 0.001\n",
        "alpha_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "alpha_optimizer = tf.keras.optimizers.Adam(alpha_lr)\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "alpha = 0.0\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "std_dev_action = 0.1\n",
        "start_size = 10000\n",
        "\n",
        "agent = Agent(200000, 64,  alpha)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "total_step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-13JOF838njy",
        "outputId": "935395c0-4aba-4564-81a3-300d98644699"
      },
      "source": [
        "for ep in range(400, 600):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "    step = 0\n",
        "    noise = tf.random.normal(mean=0.0, stddev=std_dev_action, shape=(max_steps, num_actions))\n",
        "\n",
        "    while True:\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        if agent.buffer_counter < start_size:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action, _ , _ = agent.policy(tf_prev_state, noise[step])\n",
        "            action = tf.squeeze(action)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # new_reward = reward + state[0]*10 + state[9]*10\n",
        "        agent.record((prev_state, action, reward, state, done))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        if agent.buffer_counter >= start_size:\n",
        "            agent.learn()\n",
        "            update_target(target_critic_a.variables, critic_model_a.variables, tau)\n",
        "            update_target(target_critic_b.variables, critic_model_b.variables, tau)\n",
        "\n",
        "        prev_state = state\n",
        "        step += 1\n",
        "        total_step += 1\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "\n",
        "    with episode_summary_writer.as_default():\n",
        "        tf.summary.scalar('main/reward', episodic_reward, step=ep)\n",
        "        tf.summary.scalar('main/steps', step, step=ep)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print('Episode:', ep, 'Current Reward:', episodic_reward, 'Steps:', step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 599 Current Reward: 2635.1066111801597 Steps: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WixVy9q9OGf"
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAzX0G3P27BQ"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "\n",
        "import datetime\n",
        "!rm -rf ./logs/\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "episode_log_dir = 'logs/gradient_tape/' + current_time + '/episode'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "episode_summary_writer = tf.summary.create_file_writer(episode_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKfhmQNZLVCV",
        "outputId": "deba9283-0176-4630-e230-c3e2353befff"
      },
      "source": [
        "keras.models.save_model(actor_model, 'model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIRhRJlo7Enj"
      },
      "source": [
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip3 install pyvirtualdisplay && pip install pyvirtualdisplay\n",
        "!pip install -U colabgymrender\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "directory = './video'\n",
        "video_env = Recorder(env, directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwBOvSqCNtt7"
      },
      "source": [
        "obs = video_env.reset()\n",
        "while True:\n",
        "    obs = tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
        "    _, _, action = agent.policy(obs)\n",
        "    action = tf.squeeze(action)\n",
        "    obs, reward, done, info = video_env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "        \n",
        "video_env.play()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCULNNIJGP4K"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "env = Monitor(gym.make('CartPoleContinuousBulletEnv-v0'), './video', force=True)\n",
        "state = env.reset()\n",
        "while True:\n",
        "    _ , _ , action = agent.policy(state[np.newaxis])\n",
        "    # action = np.squeeze(action)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}